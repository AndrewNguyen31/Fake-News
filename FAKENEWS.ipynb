{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements for the entire file\n",
    "# If unable to upload any of these modules/libraries, simply run 'pip install {module_name}'\n",
    "\n",
    "from datasets import Dataset\n",
    "from gensim.models import Word2Vec\n",
    "from os.path import isfile\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from spacy.lang.en import English\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will read in the three datasets we have\n",
    "- Snopes: This is a single file where we will have to separate true and false articles\n",
    "- Kaggle Fake News: Two separate files -- one containing true, the other containing false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the WELFake_Dataset data from webpage\n",
    "if not isfile(\"WELFake_Dataset.csv\"):\n",
    "    url = \"https://zenodo.org/records/4561253/files/WELFake_Dataset.csv\"\n",
    "    print(\"Downloading WELFake_dataset.csv...\")\n",
    "    filename, headers = urllib.request.urlretrieve(url, filename=\"WELFake_Dataset.csv\")\n",
    "    print(\"Download complete\\n\")\n",
    "\n",
    "# Download the FN detection datasets\n",
    "if not os.path.isdir('News_dataset'):\n",
    "    os.system('kaggle datasets download -d emineyetm/fake-news-detection-datasets')\n",
    "\n",
    "    print(\"Decompressing the file ...\")\n",
    "    os.system('unzip fake-news-detection-datasets.zip')\n",
    "    print(\"Download complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Kaggle Fake News Data (Bozkus)\n",
    "\n",
    "# Bozkus True articles\n",
    "df_fn_true = pd.read_csv(\"/content/News_dataset/True.csv\")\n",
    "print(\"Read {} sentences\".format(df_fn_true.shape))\n",
    "\n",
    "# Bozkus False Articles\n",
    "df_fn_false = pd.read_csv(\"/content/News_dataset/Fake.csv\")\n",
    "print(\"Read {} sentences\".format(df_fn_false.shape))\n",
    "\n",
    "# Load WElFake Kaggle Fake News\n",
    "df_WELFAKE = pd.read_csv(\"WELFake_Dataset.csv\")\n",
    "print(\"Read {} sentences\".format(df_WELFAKE.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the English tokenizer from spaCy and define a custom tokenizer function\n",
    "nlp = English()\n",
    "def tokenizer(s):\n",
    "    tokenize = nlp.tokenizer\n",
    "    return [token.text for token in tokenize(s)]\n",
    "\n",
    "# Initialize CountVectorizer with the custom tokenizer\n",
    "vectorizer = CountVectorizer(lowercase=True, tokenizer=tokenizer)\n",
    "\n",
    "# Function to process text data and print vocabulary size and shape of the transformed array\n",
    "def process_text_data(df, dataset_name):\n",
    "     # List to hold non-null text entries\n",
    "    no_nan_text = []\n",
    "\n",
    "    # Iterate over each text entry in the DataFrame column 'text'\n",
    "    for text in df['text']:\n",
    "        if not pd.isna(text):\n",
    "            no_nan_text.append(text)\n",
    "\n",
    "    # Fit and transform the text data using CountVectorizer\n",
    "    train_array = vectorizer.fit_transform(no_nan_text)\n",
    "\n",
    "    # Print the number of unique words in the vocabulary\n",
    "    print(f'Vocabulary Size for {dataset_name}: {len(vectorizer.get_feature_names_out())}')\n",
    "\n",
    "    # Print the shape of the transformed array (documents x features)\n",
    "    print(f'Shape of Transformed Array for {dataset_name}: {train_array.shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all the datasets and print details\n",
    "process_text_data(df_WELFAKE, \"WELFake Dataset\")\n",
    "process_text_data(df_fn_false, \"False News Dataset\")\n",
    "process_text_data(df_fn_true, \"True News Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the text columns\n",
    "true_matrix = df_fn_true[['text']].copy() #Use copy here to avoid SettingWithCopyWarning -- thinks we are not modifiying the originial DF\n",
    "false_matrix = df_fn_false[['text']].copy()\n",
    "welfake_matrix = df_WELFAKE[['text', 'label']]\n",
    "\n",
    "# Add labels to each respective matrix\n",
    "true_matrix.loc[:, 'label'] = 1\n",
    "false_matrix.loc[:, 'label'] = 0\n",
    "\n",
    "# Combine matricies\n",
    "combined_matrix = pd.concat([true_matrix, false_matrix, welfake_matrix])\n",
    "combined_matrix = combined_matrix.dropna(subset=['text'])\n",
    "\n",
    "# Shuffle each row into random order\n",
    "# Frac = 1 (shuffle 100% of datafram), drop = True (makes sure old indices aren't added back as columns)\n",
    "combined_matrix = combined_matrix.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split the dataset into train, validation, and test set\n",
    "\n",
    "# Split into train (80%) and temp (20% test)\n",
    "train_val_data, test_data = train_test_split(combined_matrix, test_size=0.2, random_state=6501)\n",
    "\n",
    "# Perform 5-fold cross-validation on validation set\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # Stratified ensures class balance in each fold\n",
    "\n",
    "X = train_val_data['text'] # Validation features\n",
    "y = train_val_data['label']  # Validation labels\n",
    "\n",
    "print(X.shape)\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.1: Logistic Regression Classifier Based on TF-IDF Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Simple Classifier\n",
    "\n",
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=30000)\n",
    "\n",
    "# Automatically tokenizes inputs\n",
    "# Fit and transform the training and validation data\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Split into train (80%) and temp (20% test)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Cross-validation with TF-IDF features\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_tfidf, y)):\n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "\n",
    "    # Split the TF-IDF features\n",
    "    X_train_fold, X_val_fold = X_tfidf[train_index], X_tfidf[val_index]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Train the model on the training fold\n",
    "    logreg.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Training accuracy\n",
    "    y_pred_train = logreg.predict(X_train_fold)\n",
    "    train_accuracy = accuracy_score(y_train_fold, y_pred_train)\n",
    "    print(f\"Train accuracy for fold {fold + 1}: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation accuracy\n",
    "    y_pred_val = logreg.predict(X_val_fold)\n",
    "    val_accuracy = accuracy_score(y_val_fold, y_pred_val)\n",
    "    print(f\"Val accuracy for fold {fold + 1}: {val_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate performance on test set\n",
    "X_test = test_data['text']\n",
    "y_test = test_data['label']\n",
    "\n",
    "# Transform the test set using the same TF-IDF vectorizer\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = logreg.predict(X_test_tfidf)\n",
    "\n",
    "#Statistics:\n",
    "\n",
    "# Test set accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Test set precision\n",
    "test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Test Set Precision: {test_precision:.4f}\")\n",
    "\n",
    "# Test set recall\n",
    "test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Test Set Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Test set F1 score\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Test Set F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1.2 Logistic Regression Classifier Based on Skip-Gram Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skipgram Model\n",
    "\n",
    "# Split sentences into words (tokenization)\n",
    "X_tokenized = X.apply(lambda x: x.split())\n",
    "\n",
    "# Step 3: Train a Skip-Gram Word2Vec model\n",
    "# sg=1 --> skipgram\n",
    "word2vec_model = Word2Vec(sentences=X_tokenized, vector_size=70, window=5, min_count=3, sg=1, workers=4, seed=6501)\n",
    "\n",
    "# Convert documents to vectors by averaging word embeddings\n",
    "def document_vector(doc):\n",
    "    # List to store word vectors found in the document\n",
    "    word_vectors = []\n",
    "\n",
    "    # Iterate over each word in the document\n",
    "    for word in doc:\n",
    "        # Check if the word is in the Word2Vec model's vocabulary (word2vec_model.wv)\n",
    "        if word in word2vec_model.wv:\n",
    "            # Append the word vector to the list\n",
    "            word_vectors.append(word2vec_model.wv[word])\n",
    "\n",
    "    # Check if the list of word vectors is not empty (i.e., at least one word was found in the vocabulary)\n",
    "    if len(word_vectors) > 0:\n",
    "        # Average the word vectors to get a single fixed-length vector for the document\n",
    "        return np.mean(word_vectors, axis=0)\n",
    "    else:\n",
    "        # If no words were found in the vocabulary --> return a zero vector of the same length as the word vectors (70 = dim)\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    [0.5, 0.2, 0.1],  # \"cat\"\n",
    "    [0.3, 0.4, 0.2],  # \"sits\"\n",
    "    [0.2, 0.1, 0.1],  # \"on\"\n",
    "    [0.4, 0.3, 0.2]   # \"mat\"\n",
    "\n",
    "    -->\n",
    "\n",
    "    [0.35, 0.25, 0.15] document represented as a vector\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "# Apply the function to create document vectors for each text in the dataset\n",
    "\n",
    "# Initialize an empty list to store the document vectors\n",
    "X_vectors = []\n",
    "\n",
    "# Iterate over each tokenized document in X_tokenized\n",
    "for doc in X_tokenized:\n",
    "    # Apply the document_vector() function to convert the document (list of words) to a vector\n",
    "    doc_vector = document_vector(doc)\n",
    "\n",
    "    # Append the resulting vector to the X_vectors list\n",
    "    X_vectors.append(doc_vector)\n",
    "\n",
    "# We convert this list into a 2D NumPy array, where each row corresponds to a document vector\n",
    "X_vectors = np.array(X_vectors)\n",
    "\n",
    "# Step 5: Perform cross-validation and logistic regression\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Cross-validation with word embeddings\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(X_vectors, y)):\n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "\n",
    "    # Split the vectorized text data\n",
    "    X_train_fold, X_val_fold = X_vectors[train_index], X_vectors[val_index]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "    # Train the logistic regression model\n",
    "    logreg.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Training accuracy\n",
    "    y_pred_train = logreg.predict(X_train_fold)\n",
    "    train_accuracy = accuracy_score(y_train_fold, y_pred_train)\n",
    "    print(f\"Train accuracy for fold {fold + 1}: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Validation accuracy\n",
    "    y_pred_val = logreg.predict(X_val_fold)\n",
    "    val_accuracy = accuracy_score(y_val_fold, y_pred_val)\n",
    "    print(f\"Val accuracy for fold {fold + 1}: {val_accuracy:.4f}\")\n",
    "\n",
    "# Step 6: Evaluate the model on the test set\n",
    "\n",
    "# Tokenize the test set and compute document vectors\n",
    "X_test_tokenized = X_test.apply(lambda x: x.split())\n",
    "\n",
    "# Same as above: Apply the function to create document vectors for each text in the dataset\n",
    "\n",
    "# Initialize an empty list to store the document vectors\n",
    "X_test_vectors = []\n",
    "\n",
    "# Iterate over each tokenized document in X_tokenized\n",
    "for doc_test in X_test_tokenized:\n",
    "    doc_vector_test = document_vector(doc_test)\n",
    "\n",
    "    X_test_vectors.append(doc_vector_test)\n",
    "\n",
    "# We convert this list into a 2D NumPy array, where each row corresponds to a document vector\n",
    "X_test_vectors = np.array(X_test_vectors)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = logreg.predict(X_test_vectors)\n",
    "\n",
    "# Statistics\n",
    "\n",
    "# Test set accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Test set precision\n",
    "test_precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Test Set Precision: {test_precision:.4f}\")\n",
    "\n",
    "# Test set recall\n",
    "test_recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Test Set Recall: {test_recall:.4f}\")\n",
    "\n",
    "# Test set F1 score\n",
    "test_f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "print(f\"Test Set F1 Score: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: The next main component of the project is Fake News Generation, which consists of the following:\n",
    "\n",
    "Explores the use of large language models (LLMs) to generate fake news articles\n",
    "Understand how these models can create realistic-looking fake news that might evade detection systems\n",
    "Generated content is then used to test the robustness of the detection models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "  tokenizer.pad_token = tokenizer.eos_token\n",
    "  model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Prepare fake news data for fine-tuning\n",
    "def prepare_data_for_training(df, sample_frac=0.3):\n",
    "  # Combine title and text for training if needed\n",
    "  df_sample = df.sample(frac=sample_frac, random_state=42)\n",
    "  texts = df_sample['text'].tolist()\n",
    "  # Tokenize all texts\n",
    "  encodings = tokenizer(texts, truncation=True, padding=True, max_length=512)\n",
    "  dataset = Dataset.from_dict(encodings)\n",
    "  return dataset\n",
    "\n",
    "# Create dataset from fake news examples\n",
    "train_dataset = prepare_data_for_training(df_fn_false, 0.3)\n",
    "\n",
    "# Set up data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "  tokenizer=tokenizer,\n",
    "  mlm=False  # We're not using masked language modeling\n",
    ")\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "  output_dir=\"./gpt2-fake-news\",\n",
    "  overwrite_output_dir=True,\n",
    "  num_train_epochs=1,\n",
    "  per_device_train_batch_size=6,\n",
    "  save_steps=5000,\n",
    "  save_total_limit=2,\n",
    "  logging_steps=500,\n",
    "  report_to=[],\n",
    "  fp16=True\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  data_collator=data_collator,\n",
    "  train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate fake news with the fine-tuned model\n",
    "def generate_fake_news(prompt, max_length=200):\n",
    "  inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "  outputs = model.generate(\n",
    "      inputs,\n",
    "      max_length=max_length,\n",
    "      num_return_sequences=1,\n",
    "      temperature=0.9,  # Control randomness (higher = more random)\n",
    "      top_k=50,        # Limit vocabulary choices\n",
    "      top_p=0.95,      # Nucleus sampling\n",
    "      no_repeat_ngram_size=2,  # Prevent repetition of phrases\n",
    "      do_sample=True\n",
    "  )\n",
    "  text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "  return text\n",
    "\n",
    "# Create fake news articles\n",
    "# Create fake news articles with additional prompts\n",
    "prompts = [\n",
    "    \"Breaking News: Scientists discover\",\n",
    "    \"Latest Update: Government announces\",\n",
    "    \"Exclusive: Celebrity reveals\",\n",
    "    \"Sports: Team wins championship\",\n",
    "    \"Technology: New gadget released\",\n",
    "    \"Health Alert: New virus outbreak\",\n",
    "    \"Economy: Stock market hits record high\",\n",
    "    \"Politics: Leader resigns amid scandal\",\n",
    "    \"Environment: Major climate change initiative\",\n",
    "    \"Education: Schools adopt new curriculum\",\n",
    "    \"Travel: New airline routes announced\",\n",
    "    \"Science: Researchers develop new vaccine\",\n",
    "    \"Crime: Suspect arrested in major case\",\n",
    "    \"Entertainment: Film wins top award\",\n",
    "    \"Weather: Severe storm warning issued\",\n",
    "    \"Business: Company announces major merger\",\n",
    "    \"Culture: Festival celebrates diversity\",\n",
    "    \"Space: New planet discovered by astronomers\",\n",
    "    \"Technology: Breakthrough in artificial intelligence\",\n",
    "    \"Health: Study reveals benefits of new diet\"\n",
    "]\n",
    "generated_articles = []\n",
    "for prompt in prompts:\n",
    "  fake_news_article = generate_fake_news(prompt)\n",
    "  generated_articles.append({'prompt': prompt, 'article': fake_news_article})\n",
    "\n",
    "# Save the generated articles to a DataFrame\n",
    "df_fake_news = pd.DataFrame(generated_articles)\n",
    "\n",
    "# Save the DataFrame to a CSV file for later testing\n",
    "df_fake_news.to_csv('generated_fake_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 20 articles from the true dataset then combine datasets\n",
    "df_sampled_true = df_fn_true.sample(n=20, random_state=42)['text']\n",
    "df_fake_news = pd.read_csv('generated_fake_news.csv')['article']\n",
    "df_combined = pd.concat([df_sampled_true, df_fake_news], ignore_index=True)\n",
    "\n",
    "# Extract the articles for testing\n",
    "X_combined = df_combined\n",
    "y_combined = [1] * len(df_sampled_true) + [0] * len(df_fake_news)\n",
    "\n",
    "# TF-IDF Vectorizer Testing\n",
    "# Transform the combined articles using the same TF-IDF vectorizer\n",
    "X_combined_tfidf = tfidf_vectorizer.transform(X_combined)\n",
    "\n",
    "# Make predictions on the combined articles using the TF-IDF based logistic regression model\n",
    "y_combined_pred_tfidf = logreg.predict(X_combined_tfidf)\n",
    "\n",
    "# Calculate and print evaluation metrics for TF-IDF model\n",
    "print(\"TF-IDF Logistic Regression Evaluation for Combined Articles:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_combined, y_combined_pred_tfidf):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_combined, y_combined_pred_tfidf, average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_combined, y_combined_pred_tfidf, average='weighted'):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_combined, y_combined_pred_tfidf, average='weighted'):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip-Gram Embeddings Testing (if applicable)\n",
    "# Tokenize the combined articles\n",
    "X_combined_tokenized = X_combined.apply(lambda x: x.split())\n",
    "\n",
    "# Convert the tokenized articles to vectors using the document_vector function\n",
    "X_combined_vectors = np.array([document_vector(doc) for doc in X_combined_tokenized])\n",
    "\n",
    "# Make predictions on the combined articles using the Skip-Gram based logistic regression model\n",
    "y_combined_pred_skipgram = logreg.predict(X_combined_vectors)\n",
    "\n",
    "# Calculate and print evaluation metrics for Skip-Gram model\n",
    "print(\"Skip-Gram Logistic Regression Evaluation for Combined Articles:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_combined, y_combined_pred_skipgram):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_combined, y_combined_pred_skipgram, average='weighted'):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_combined, y_combined_pred_skipgram, average='weighted'):.4f}\")\n",
    "print(f\"F1 Score: {f1_score(y_combined, y_combined_pred_skipgram, average='weighted'):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
